{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "title_and_objectives"
      },
      "source": [
        "# PCB Defect Detection with YOLOv12\n",
        "\n",
        "## Business Objective\n",
        "\n",
        "Electronics manufacturers face high scrap costs and yield losses due to rigid, legacy Automated Optical Inspection (AOI) systems that generate excessive false positives. This notebook demonstrates training a state-of-the-art **YOLOv12** object detection model directly inside Snowflake using GPU compute, enabling adaptive defect detection without moving sensitive image data off-platform.\n",
        "\n",
        "## Technical Approach\n",
        "\n",
        "We use the **Deep PCB** dataset containing 6 defect classes (open, short, mousebite, spur, copper, pin-hole) to train a YOLOv12 model on Snowflake's Container Runtime with GPU acceleration. The trained model is logged to **Snowflake's Model Registry** using the CustomModel API, enabling versioning, SQL inference, and SPCS deployment.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "After completing this notebook, you will understand:\n",
        "1. How to use Snowflake Notebooks on Container Runtime with GPU compute pools\n",
        "2. Converting industrial defect datasets to YOLO format\n",
        "3. Training YOLOv12 for object detection tasks\n",
        "4. Logging custom models to Snowflake's Model Registry using the CustomModel API\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- **Mathematics**: Basic understanding of neural networks and loss functions\n",
        "- **ML Concepts**: Familiarity with object detection, bounding boxes, IoU\n",
        "- **Python**: Intermediate Python and PyTorch experience\n",
        "- **Domain**: Basic PCB manufacturing knowledge helpful but not required\n",
        "\n",
        "### Required Setup (Run Once)\n",
        "\n",
        "Before executing this notebook, an administrator must configure external network access for GitHub:\n",
        "\n",
        "```sql\n",
        "-- Run sql/03_network_egress.sql as ACCOUNTADMIN, then:\n",
        "ALTER NOTEBOOK CV_DEFECT_DETECTION_YOLO_NOTEBOOK\n",
        "  SET EXTERNAL_ACCESS_INTEGRATIONS = (github_access_integration);\n",
        "```\n",
        "\n",
        "This enables the notebook to download the Deep PCB dataset from GitHub.\n",
        "\n",
        "## Notebook Structure\n",
        "\n",
        "| Section | Purpose |\n",
        "|---------|----------|\n",
        "| 1. Title & Objectives | Frame the problem |\n",
        "| 2. Environment Setup | Install packages, imports, session |\n",
        "| 3. Data Loading | Download from Snowflake stage |\n",
        "| 4. Data Exploration | Visualize defect distribution |\n",
        "| 5. Model/Algorithm | Convert to YOLO format, explain architecture |\n",
        "| 6. Execution | Train YOLOv12 with diagnostics |\n",
        "| 7. Evaluation | Loss curves, sample predictions |\n",
        "| 8. Production Output | Log to Model Registry, write inference logs |\n",
        "| 9. Key Takeaways | Summary and next steps |\n",
        "\n",
        "## Output\n",
        "\n",
        "- Trained YOLOv12 model logged to Snowflake Model Registry as `YOLO_PCB_DEFECT_DETECTOR`\n",
        "- Model also saved to `@MODEL_STAGE/models/yolov12_pcb/yolo_best.pt` for Vision Lab\n",
        "- Inference results written to `DEFECT_LOGS` table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "environment_setup_header"
      },
      "source": [
        "---\n",
        "## 2. Environment Setup\n",
        "\n",
        "Install required packages and configure the runtime environment. We find the pip executable in the Container Runtime and install PyPI packages (ultralytics, supervision, opencv-python-headless).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "install_packages_and_imports"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PACKAGE INSTALLATION\n",
        "# =============================================================================\n",
        "# Install PyPI packages using pip (SPCS Container Runtime)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "print(\"Installing packages...\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "# Install packages using os.system (equivalent to !pip install in Jupyter)\n",
        "packages = [\"ultralytics\", \"supervision\", \"opencv-python-headless\", \"snowflake-ml-python\", \"torch\"]\n",
        "\n",
        "for pkg in packages:\n",
        "    print(f\"\\n  Installing {pkg}...\")\n",
        "    exit_code = os.system(f\"pip install {pkg} --quiet\")\n",
        "    if exit_code != 0:\n",
        "        raise RuntimeError(f\"Failed to install {pkg}: exit code {exit_code}\")\n",
        "    print(f\"  [OK] {pkg} installed\")\n",
        "\n",
        "# Refresh import system\n",
        "importlib.invalidate_caches()\n",
        "print(\"\\n[OK] Package installation complete\")\n",
        "\n",
        "# =============================================================================\n",
        "# IMPORTS - All imports in same cell after install\n",
        "# =============================================================================\n",
        "\n",
        "# Standard library\n",
        "import glob\n",
        "import uuid\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# Data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import yaml\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ML/Computer Vision - import after pip install\n",
        "from ultralytics import YOLO\n",
        "from snowflake.ml.registry import Registry\n",
        "from snowflake.ml.modeling.distributors.pytorch import PyTorchDistributor, PyTorchScalingConfig, WorkerResourceConfig\n",
        "from snowflake.ml.data.sharded_data_connector import ShardedDataConnector\n",
        "\n",
        "# Snowflake\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "from snowflake.snowpark.functions import col\n",
        "\n",
        "print(\"[OK] All imports successful\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "snowflake_session_setup"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SNOWFLAKE SESSION SETUP\n",
        "# =============================================================================\n",
        "# Get active session from Container Runtime context\n",
        "\n",
        "session = get_active_session()\n",
        "\n",
        "# Verify context\n",
        "print(\"Snowflake Session Info:\")\n",
        "print(f\"  Database: {session.get_current_database()}\")\n",
        "print(f\"  Schema: {session.get_current_schema()}\")\n",
        "print(f\"  Warehouse: {session.get_current_warehouse()}\")\n",
        "print(f\"  Role: {session.get_current_role()}\")\n",
        "\n",
        "# Helper function for fail-fast queries\n",
        "def execute_query_helper(query: str, name: str = \"query\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Execute a SQL query with fail-fast error handling.\n",
        "    \n",
        "    Args:\n",
        "        query: SQL query string\n",
        "        name: Descriptive name for error messages\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with query results\n",
        "    \n",
        "    Raises:\n",
        "        RuntimeError: If query fails or returns None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = session.sql(query).to_pandas()\n",
        "        if result is None:\n",
        "            raise RuntimeError(f\"Query '{name}' returned None\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Query '{name}' failed: {e}\") from e\n",
        "\n",
        "print(\"\\n[OK] Session configured with fail-fast query helper\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "visualization_config"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZATION SETUP - Dark Theme\n",
        "# =============================================================================\n",
        "# Configure matplotlib for Snowflake-inspired dark theme\n",
        "\n",
        "plt.style.use('dark_background')\n",
        "plt.rcParams.update({\n",
        "    # Background colors (soft dark gray, not pure black)\n",
        "    'figure.facecolor': '#121212',\n",
        "    'axes.facecolor': '#121212',\n",
        "    \n",
        "    # Text colors (off-white to reduce glare)\n",
        "    'text.color': '#E5E5E7',\n",
        "    'axes.labelcolor': '#E5E5E7',\n",
        "    'xtick.color': '#A1A1A6',\n",
        "    'ytick.color': '#A1A1A6',\n",
        "    \n",
        "    # Grid and axes (subtle, not distracting)\n",
        "    'axes.edgecolor': '#3A3A3C',\n",
        "    'grid.color': '#2C2C2E',\n",
        "    'grid.alpha': 0.6,\n",
        "    \n",
        "    # Figure quality and sizing\n",
        "    'figure.dpi': 150,\n",
        "    'savefig.dpi': 200,\n",
        "    'figure.figsize': (12, 6),\n",
        "    \n",
        "    # Font configuration\n",
        "    'font.family': 'sans-serif',\n",
        "    'font.size': 11,\n",
        "    'axes.titlesize': 14,\n",
        "    'axes.labelsize': 12,\n",
        "})\n",
        "\n",
        "# Colorblind-safe palette for dark backgrounds\n",
        "COLORS = ['#64D2FF', '#FF9F0A', '#5AC8FA', '#FFD60A', '#11567F', '#30D158']\n",
        "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=COLORS)\n",
        "\n",
        "print(\"[OK] Dark theme visualization configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "data_loading_header"
      },
      "source": [
        "---\n",
        "## 3. Data Loading\n",
        "\n",
        "Load the Deep PCB dataset using a hybrid approach:\n",
        "1. **Fast path**: If data is cached in `@MODEL_STAGE/raw/deeppcb/`, download from stage\n",
        "2. **First-time**: If stage is empty, clone directly from GitHub using sparse checkout\n",
        "\n",
        "This approach eliminates the need for manual data upload during deployment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "download_dataset"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HYBRID DATA LOADING: Stage Cache with Git Fallback\n",
        "# =============================================================================\n",
        "import subprocess\n",
        "\n",
        "# Local paths in ephemeral container storage\n",
        "RAW_DIR = \"/tmp/raw_download\"\n",
        "DATASET_DIR = \"/tmp/deeppcb_dataset\"\n",
        "IMG_DIR = os.path.join(DATASET_DIR, \"images\")\n",
        "LBL_DIR = os.path.join(DATASET_DIR, \"labels\")\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(RAW_DIR, exist_ok=True)\n",
        "os.makedirs(IMG_DIR, exist_ok=True)\n",
        "os.makedirs(LBL_DIR, exist_ok=True)\n",
        "\n",
        "# Check if data exists in stage (look for .jpg files)\n",
        "print(\"Checking Snowflake stage for cached dataset...\")\n",
        "try:\n",
        "    stage_files = session.sql(\n",
        "        \"LIST @MODEL_STAGE/raw/deeppcb/ PATTERN='.*\\\\.jpg'\"\n",
        "    ).collect()\n",
        "except Exception:\n",
        "    stage_files = []\n",
        "\n",
        "if len(stage_files) > 10:\n",
        "    # Stage has data - download from stage (preserving directory structure)\n",
        "    print(f\"[OK] Found {len(stage_files)} cached files in stage\")\n",
        "    print(\"Downloading from Snowflake stage (preserving structure)...\")\n",
        "    \n",
        "    for row in stage_files:\n",
        "        # row['name'] looks like: model_stage/raw/deeppcb/group00041/00041/00041001_temp.jpg\n",
        "        stage_path = row['name']\n",
        "        # Extract relative path after raw/deeppcb/\n",
        "        if 'raw/deeppcb/' in stage_path:\n",
        "            rel_path = stage_path.split('raw/deeppcb/')[-1]\n",
        "            local_path = os.path.join(RAW_DIR, rel_path)\n",
        "            \n",
        "            # Create parent directories to preserve structure\n",
        "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "            \n",
        "            # Download single file to its proper location\n",
        "            session.file.get(f\"@MODEL_STAGE/raw/deeppcb/{rel_path}\", os.path.dirname(local_path))\n",
        "    \n",
        "    print(f\"[OK] Downloaded {len(stage_files)} files to {RAW_DIR}\")\n",
        "else:\n",
        "    # Stage empty - clone from GitHub (first-time setup)\n",
        "    print(\"Stage empty. Cloning from GitHub (first-time setup)...\")\n",
        "    \n",
        "    CLONE_DIR = \"/tmp/DeepPCB\"\n",
        "    \n",
        "    # Verify git is available\n",
        "    git_check = subprocess.run(['which', 'git'], capture_output=True)\n",
        "    if git_check.returncode != 0:\n",
        "        raise RuntimeError(\n",
        "            \"git not found in container. \"\n",
        "            \"Please upload data to @MODEL_STAGE/raw/deeppcb/ using deploy.sh --only-data\"\n",
        "        )\n",
        "    \n",
        "    # Clean previous clone attempt if exists\n",
        "    if os.path.exists(CLONE_DIR):\n",
        "        shutil.rmtree(CLONE_DIR)\n",
        "    \n",
        "    # Git sparse clone (only PCBData folder)\n",
        "    print(\"  Running git sparse clone...\")\n",
        "    subprocess.run([\n",
        "        'git', 'clone', '--filter=blob:none', '--sparse', '--depth=1',\n",
        "        'https://github.com/tangsanli5201/DeepPCB.git', CLONE_DIR\n",
        "    ], check=True)\n",
        "    \n",
        "    subprocess.run(\n",
        "        ['git', 'sparse-checkout', 'set', 'PCBData'],\n",
        "        cwd=CLONE_DIR, check=True\n",
        "    )\n",
        "    \n",
        "    # Point RAW_DIR to cloned data\n",
        "    RAW_DIR = os.path.join(CLONE_DIR, \"PCBData\")\n",
        "    print(f\"[OK] Cloned to {RAW_DIR}\")\n",
        "    \n",
        "    # Cache to stage for future runs and Vision Lab access\n",
        "    print(\"ðŸ“¤ Caching dataset to Snowflake stage for future runs...\")\n",
        "    upload_count = 0\n",
        "    for group_dir in Path(RAW_DIR).glob(\"group*\"):\n",
        "        group_name = group_dir.name\n",
        "        for sub_dir in group_dir.iterdir():\n",
        "            if sub_dir.is_dir():\n",
        "                sub_name = sub_dir.name\n",
        "                for file_path in sub_dir.glob(\"*\"):\n",
        "                    if file_path.is_file():\n",
        "                        stage_path = f\"@MODEL_STAGE/raw/deeppcb/{group_name}/{sub_name}/\"\n",
        "                        session.file.put(str(file_path), stage_path, auto_compress=False, overwrite=True)\n",
        "                        upload_count += 1\n",
        "    print(f\"[OK] Cached {upload_count} files to @MODEL_STAGE/raw/deeppcb/\")\n",
        "\n",
        "# List downloaded/cloned files\n",
        "downloaded_files = list(Path(RAW_DIR).rglob(\"*\"))\n",
        "print(f\"\\nðŸ“ Found {len(downloaded_files)} files/directories\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "data_exploration_header"
      },
      "source": [
        "---\n",
        "## 4. Data Exploration\n",
        "\n",
        "Explore the Deep PCB dataset to understand the defect distribution and image characteristics before training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "explore_dataset_structure"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EXPLORE DEEP PCB DATASET STRUCTURE\n",
        "# =============================================================================\n",
        "\n",
        "# Deep PCB class mapping (1-indexed in original dataset)\n",
        "# Map to YOLO 0-indexed classes\n",
        "CLASS_NAMES = {\n",
        "    0: 'open',\n",
        "    1: 'short', \n",
        "    2: 'mousebite',\n",
        "    3: 'spur',\n",
        "    4: 'copper',\n",
        "    5: 'pin-hole'\n",
        "}\n",
        "\n",
        "# Original Deep PCB uses 1-6 indexing\n",
        "DEEPPCB_TO_YOLO = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5}\n",
        "\n",
        "print(\"Deep PCB Defect Classes:\")\n",
        "print(\"=\"*50)\n",
        "for idx, name in CLASS_NAMES.items():\n",
        "    print(f\"  Class {idx}: {name}\")\n",
        "\n",
        "# Find image and annotation files\n",
        "image_files = list(Path(RAW_DIR).rglob(\"*.jpg\")) + list(Path(RAW_DIR).rglob(\"*.png\"))\n",
        "txt_files = list(Path(RAW_DIR).rglob(\"*.txt\"))\n",
        "\n",
        "print(f\"\\nðŸ“ Dataset Statistics:\")\n",
        "print(f\"  Image files: {len(image_files)}\")\n",
        "print(f\"  Annotation files: {len(txt_files)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "defect_distribution_visualization"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZE DEFECT CLASS DISTRIBUTION\n",
        "# =============================================================================\n",
        "\n",
        "# Count defects by class from annotation files\n",
        "class_counts = {name: 0 for name in CLASS_NAMES.values()}\n",
        "\n",
        "for txt_path in txt_files:\n",
        "    try:\n",
        "        with open(txt_path, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) >= 5:\n",
        "                    # Deep PCB format: x1 y1 x2 y2 class_id\n",
        "                    cls_id = int(parts[4])\n",
        "                    if cls_id in DEEPPCB_TO_YOLO:\n",
        "                        yolo_cls = DEEPPCB_TO_YOLO[cls_id]\n",
        "                        class_counts[CLASS_NAMES[yolo_cls]] += 1\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "# Create bar chart\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "classes = list(class_counts.keys())\n",
        "counts = list(class_counts.values())\n",
        "\n",
        "bars = ax.bar(classes, counts, color=COLORS[:len(classes)], edgecolor='white', linewidth=0.5)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, count in zip(bars, counts):\n",
        "    if count > 0:\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
        "                str(count), ha='center', va='bottom', fontsize=11, color='#E5E5E7')\n",
        "\n",
        "ax.set_xlabel('Defect Class')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Deep PCB Defect Distribution', fontsize=14, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/defect_distribution.png', dpi=150, bbox_inches='tight', facecolor='#121212')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nTotal defects found: {sum(counts)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "sample_images_visualization"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZE SAMPLE PCB IMAGES WITH DEFECT ANNOTATIONS\n",
        "# =============================================================================\n",
        "# Display raw images with ground truth bounding boxes to understand what\n",
        "# the model will learn to detect. This helps validate annotation quality\n",
        "# and provides intuition for defect appearance.\n",
        "\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def draw_bbox_on_image(ax, img_path, txt_path, class_colors):\n",
        "    \"\"\"\n",
        "    Draw an image with its ground truth bounding boxes overlaid.\n",
        "    \n",
        "    Args:\n",
        "        ax: Matplotlib axis to draw on\n",
        "        img_path: Path to the image file\n",
        "        txt_path: Path to the annotation file (Deep PCB format)\n",
        "        class_colors: Dict mapping class names to colors\n",
        "    \"\"\"\n",
        "    # Load image\n",
        "    img = Image.open(img_path)\n",
        "    img_width, img_height = img.size\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    \n",
        "    # Parse annotations (Deep PCB format: x1 y1 x2 y2 class_id)\n",
        "    if txt_path.exists():\n",
        "        with open(txt_path, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) >= 5:\n",
        "                    x1, y1, x2, y2, cls_id = map(int, parts[:5])\n",
        "                    if cls_id in DEEPPCB_TO_YOLO:\n",
        "                        yolo_cls = DEEPPCB_TO_YOLO[cls_id]\n",
        "                        class_name = CLASS_NAMES[yolo_cls]\n",
        "                        color = class_colors.get(class_name, '#FFFFFF')\n",
        "                        \n",
        "                        # Draw rectangle\n",
        "                        rect = patches.Rectangle(\n",
        "                            (x1, y1), x2-x1, y2-y1,\n",
        "                            linewidth=2, edgecolor=color, facecolor='none'\n",
        "                        )\n",
        "                        ax.add_patch(rect)\n",
        "                        \n",
        "                        # Add label\n",
        "                        ax.text(x1, y1-5, class_name, color=color, fontsize=8,\n",
        "                               bbox=dict(boxstyle='round,pad=0.2', facecolor='#121212', alpha=0.7))\n",
        "\n",
        "# Color mapping for defect classes\n",
        "CLASS_COLORS = {\n",
        "    'open': '#FF6B6B',      # Red-coral\n",
        "    'short': '#4ECDC4',     # Teal\n",
        "    'mousebite': '#FFE66D', # Yellow\n",
        "    'spur': '#FF9F0A',      # Orange\n",
        "    'copper': '#64D2FF',    # Blue\n",
        "    'pin-hole': '#30D158'   # Green\n",
        "}\n",
        "\n",
        "# Find sample images with annotations\n",
        "sample_pairs = []\n",
        "for img_path in Path(RAW_DIR).rglob(\"*.jpg\"):\n",
        "    # Find corresponding annotation file\n",
        "    img_name = img_path.stem\n",
        "    base_id = img_name.replace('_temp', '').replace('_test', '')\n",
        "    img_dir = img_path.parent\n",
        "    parent_dir = img_dir.parent\n",
        "    dir_name = img_dir.name\n",
        "    not_dir = parent_dir / f\"{dir_name}_not\"\n",
        "    txt_path = not_dir / f\"{base_id}.txt\"\n",
        "    \n",
        "    if txt_path.exists():\n",
        "        sample_pairs.append((img_path, txt_path))\n",
        "        if len(sample_pairs) >= 6:\n",
        "            break\n",
        "\n",
        "# Create visualization grid\n",
        "if sample_pairs:\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, (img_path, txt_path) in enumerate(sample_pairs):\n",
        "        draw_bbox_on_image(axes[idx], img_path, txt_path, CLASS_COLORS)\n",
        "        axes[idx].set_title(f'{img_path.name}', fontsize=10)\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for idx in range(len(sample_pairs), 6):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    # Add legend\n",
        "    legend_elements = [patches.Patch(facecolor=color, edgecolor=color, label=name)\n",
        "                       for name, color in CLASS_COLORS.items()]\n",
        "    fig.legend(handles=legend_elements, loc='lower center', ncol=6, \n",
        "               fontsize=10, framealpha=0.8)\n",
        "    \n",
        "    plt.suptitle('Sample PCB Images with Ground Truth Annotations', \n",
        "                 fontsize=14, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/tmp/sample_images_annotated.png', dpi=150, bbox_inches='tight', facecolor='#121212')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nDisplayed {len(sample_pairs)} sample images with annotations\")\n",
        "else:\n",
        "    print(\"[WARN] No image-annotation pairs found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "dataset_statistics_visualization"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATASET STATISTICS: Image Dimensions & Defects Per Image\n",
        "# =============================================================================\n",
        "# Understanding the dataset characteristics helps inform training parameters\n",
        "# and identify potential issues (e.g., varying image sizes, class imbalance).\n",
        "\n",
        "# Collect image dimensions and defect counts\n",
        "image_dims = []\n",
        "defects_per_image = []\n",
        "defect_sizes = []\n",
        "\n",
        "for img_path in Path(RAW_DIR).rglob(\"*.jpg\"):\n",
        "    try:\n",
        "        # Get image dimensions\n",
        "        with Image.open(img_path) as img:\n",
        "            w, h = img.size\n",
        "            image_dims.append((w, h))\n",
        "        \n",
        "        # Find annotation file\n",
        "        img_name = img_path.stem\n",
        "        base_id = img_name.replace('_temp', '').replace('_test', '')\n",
        "        img_dir = img_path.parent\n",
        "        parent_dir = img_dir.parent\n",
        "        dir_name = img_dir.name\n",
        "        not_dir = parent_dir / f\"{dir_name}_not\"\n",
        "        txt_path = not_dir / f\"{base_id}.txt\"\n",
        "        \n",
        "        # Count defects and their sizes\n",
        "        if txt_path.exists():\n",
        "            defect_count = 0\n",
        "            with open(txt_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) >= 5:\n",
        "                        x1, y1, x2, y2 = map(int, parts[:4])\n",
        "                        defect_count += 1\n",
        "                        # Store defect size (area in pixels)\n",
        "                        defect_sizes.append((x2 - x1) * (y2 - y1))\n",
        "            defects_per_image.append(defect_count)\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "# Plot 1: Image dimensions\n",
        "if image_dims:\n",
        "    widths = [d[0] for d in image_dims]\n",
        "    heights = [d[1] for d in image_dims]\n",
        "    axes[0].scatter(widths, heights, alpha=0.5, c=COLORS[0], s=20)\n",
        "    axes[0].set_xlabel('Width (pixels)')\n",
        "    axes[0].set_ylabel('Height (pixels)')\n",
        "    axes[0].set_title('Image Dimensions Distribution', fontweight='bold')\n",
        "    axes[0].grid(alpha=0.3)\n",
        "    \n",
        "    # Add annotation for common size\n",
        "    from collections import Counter\n",
        "    dim_counts = Counter(image_dims)\n",
        "    most_common_dim, count = dim_counts.most_common(1)[0]\n",
        "    axes[0].annotate(f'Most common: {most_common_dim[0]}x{most_common_dim[1]}\\n({count} images)',\n",
        "                     xy=most_common_dim, fontsize=9, color='#E5E5E7',\n",
        "                     xytext=(10, -30), textcoords='offset points',\n",
        "                     bbox=dict(boxstyle='round', facecolor='#2C2C2E', alpha=0.8))\n",
        "\n",
        "# Plot 2: Defects per image\n",
        "if defects_per_image:\n",
        "    axes[1].hist(defects_per_image, bins=range(0, max(defects_per_image)+2), \n",
        "                 color=COLORS[1], edgecolor='white', linewidth=0.5, alpha=0.8)\n",
        "    axes[1].set_xlabel('Number of Defects')\n",
        "    axes[1].set_ylabel('Number of Images')\n",
        "    axes[1].set_title('Defects Per Image Distribution', fontweight='bold')\n",
        "    axes[1].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add statistics\n",
        "    avg_defects = np.mean(defects_per_image)\n",
        "    axes[1].axvline(avg_defects, color='#FF6B6B', linestyle='--', linewidth=2, label=f'Mean: {avg_defects:.1f}')\n",
        "    axes[1].legend()\n",
        "\n",
        "# Plot 3: Defect sizes\n",
        "if defect_sizes:\n",
        "    axes[2].hist(defect_sizes, bins=30, color=COLORS[2], edgecolor='white', linewidth=0.5, alpha=0.8)\n",
        "    axes[2].set_xlabel('Defect Area (pixelsÂ²)')\n",
        "    axes[2].set_ylabel('Count')\n",
        "    axes[2].set_title('Defect Size Distribution', fontweight='bold')\n",
        "    axes[2].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add median line\n",
        "    median_size = np.median(defect_sizes)\n",
        "    axes[2].axvline(median_size, color='#FF6B6B', linestyle='--', linewidth=2, label=f'Median: {median_size:.0f}pxÂ²')\n",
        "    axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/dataset_statistics.png', dpi=150, bbox_inches='tight', facecolor='#121212')\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nDataset Summary Statistics:\")\n",
        "print(f\"  Total images analyzed: {len(image_dims)}\")\n",
        "print(f\"  Images with annotations: {len(defects_per_image)}\")\n",
        "if defects_per_image:\n",
        "    print(f\"  Defects per image: min={min(defects_per_image)}, max={max(defects_per_image)}, avg={np.mean(defects_per_image):.1f}\")\n",
        "if defect_sizes:\n",
        "    print(f\"  Defect sizes (pxÂ²): min={min(defect_sizes)}, max={max(defect_sizes)}, median={np.median(defect_sizes):.0f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "data_conversion_explainer"
      },
      "source": [
        "---\n",
        "## 5. Model/Algorithm\n",
        "\n",
        "Before implementing the model, we explain the data conversion process and YOLOv12 architecture.\n",
        "\n",
        "### Deep PCB to YOLO Format Conversion\n",
        "\n",
        "**What is the conversion?**\n",
        "\n",
        "The Deep PCB dataset uses absolute pixel coordinates for bounding boxes, while YOLO requires normalized center-based coordinates.\n",
        "\n",
        "**Deep PCB Format:** `x1 y1 x2 y2 class_id` (absolute pixels)\n",
        "\n",
        "**YOLO Format:** `class_id x_center y_center width height` (normalized 0-1)\n",
        "\n",
        "### Visual Representation\n",
        "\n",
        "```\n",
        "  Deep PCB Format (absolute px)          YOLO Format (normalized 0-1)\n",
        "  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "                                         \n",
        "  Image (WÃ—H pixels)                     Image (1.0 Ã— 1.0 normalized)\n",
        "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "  â”‚                         â”‚            â”‚                         â”‚\n",
        "  â”‚  (x1,y1)                â”‚            â”‚                         â”‚\n",
        "  â”‚     â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚            â”‚         â—               â”‚\n",
        "  â”‚     â”‚             â”‚     â”‚    â†’â†’â†’     â”‚      (cx,cy)            â”‚\n",
        "  â”‚     â”‚   DEFECT    â”‚     â”‚            â”‚         w               â”‚\n",
        "  â”‚     â”‚             â”‚     â”‚            â”‚       â—„â”€â”€â”€â–º             â”‚\n",
        "  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—     â”‚            â”‚         h               â”‚\n",
        "  â”‚              (x2,y2)    â”‚            â”‚                         â”‚\n",
        "  â”‚                         â”‚            â”‚                         â”‚\n",
        "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "  \n",
        "  Corner-based coordinates               Center-based, normalized\n",
        "  (x1, y1) = top-left corner             cx = (x1+x2)/(2W)  âˆˆ [0,1]\n",
        "  (x2, y2) = bottom-right corner         cy = (y1+y2)/(2H)  âˆˆ [0,1]\n",
        "                                         w  = (x2-x1)/W     âˆˆ [0,1]\n",
        "                                         h  = (y2-y1)/H     âˆˆ [0,1]\n",
        "```\n",
        "\n",
        "### Why Normalize?\n",
        "\n",
        "1. **Scale Invariance**: Normalized coordinates work regardless of image resolution\n",
        "2. **Consistent Range**: All values in [0,1] simplify neural network training\n",
        "3. **Anchor-based Detection**: YOLO uses center-based anchors for prediction\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "Given image dimensions $(W, H)$ and bounding box $(x_1, y_1, x_2, y_2)$:\n",
        "\n",
        "$$x_{center} = \\frac{x_1 + x_2}{2W}$$\n",
        "\n",
        "$$y_{center} = \\frac{y_1 + y_2}{2H}$$\n",
        "\n",
        "$$width = \\frac{x_2 - x_1}{W}$$\n",
        "\n",
        "$$height = \\frac{y_2 - y_1}{H}$$\n",
        "\n",
        "All values are normalized to the range $[0, 1]$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "convert_to_yolo_format_cell"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONVERT DEEP PCB TO YOLO FORMAT\n",
        "# =============================================================================\n",
        "\n",
        "def convert_bbox_to_yolo(img_width: int, img_height: int, \n",
        "                         x1: int, y1: int, x2: int, y2: int) -> tuple:\n",
        "    \"\"\"\n",
        "    Convert absolute pixel coordinates to YOLO normalized format.\n",
        "    \n",
        "    Args:\n",
        "        img_width: Image width in pixels\n",
        "        img_height: Image height in pixels\n",
        "        x1, y1: Top-left corner coordinates\n",
        "        x2, y2: Bottom-right corner coordinates\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (x_center, y_center, width, height) normalized to [0,1]\n",
        "        Each value is a float in range [0.0, 1.0]\n",
        "    \n",
        "    Example:\n",
        "        >>> convert_bbox_to_yolo(640, 480, 100, 50, 200, 150)\n",
        "        (0.234, 0.208, 0.156, 0.208)  # (cx, cy, w, h) normalized\n",
        "    \"\"\"\n",
        "    # Calculate box dimensions (in pixels)\n",
        "    box_width = x2 - x1    # int: width in pixels\n",
        "    box_height = y2 - y1   # int: height in pixels\n",
        "    \n",
        "    # Calculate center point (in pixels)\n",
        "    x_center = x1 + (box_width / 2.0)   # float: center x in pixels\n",
        "    y_center = y1 + (box_height / 2.0)  # float: center y in pixels\n",
        "    \n",
        "    # Normalize to [0, 1] by dividing by image dimensions\n",
        "    x_center_norm = x_center / img_width   # float âˆˆ [0, 1]\n",
        "    y_center_norm = y_center / img_height  # float âˆˆ [0, 1]\n",
        "    width_norm = box_width / img_width     # float âˆˆ [0, 1]\n",
        "    height_norm = box_height / img_height  # float âˆˆ [0, 1]\n",
        "    \n",
        "    return x_center_norm, y_center_norm, width_norm, height_norm  # tuple(4,)\n",
        "\n",
        "\n",
        "def process_deeppcb_dataset(raw_dir: str, img_out_dir: str, lbl_out_dir: str) -> int:\n",
        "    \"\"\"\n",
        "    Process Deep PCB dataset and convert to YOLO format.\n",
        "    \n",
        "    Deep PCB structure:\n",
        "      - Images in: group*/XXXXX/XXXXX001_temp.jpg (or _test.jpg)\n",
        "      - Annotations in: group*/XXXXX_not/XXXXX001.txt\n",
        "    \n",
        "    Returns:\n",
        "        Number of images processed\n",
        "    \"\"\"\n",
        "    processed = 0\n",
        "    \n",
        "    # Find all image files\n",
        "    image_files = list(Path(raw_dir).rglob(\"*.jpg\"))\n",
        "    print(f\"  Found {len(image_files)} image files\")\n",
        "    \n",
        "    for img_path in image_files:\n",
        "        # Deep PCB naming: 00041001_temp.jpg -> annotation is 00041001.txt\n",
        "        # Located in sibling _not directory\n",
        "        img_name = img_path.stem  # e.g., \"00041001_temp\"\n",
        "        \n",
        "        # Remove _temp or _test suffix to get base ID\n",
        "        base_id = img_name.replace('_temp', '').replace('_test', '')\n",
        "        \n",
        "        # Find annotation in _not directory (sibling to image directory)\n",
        "        img_dir = img_path.parent  # e.g., .../group00041/00041/\n",
        "        parent_dir = img_dir.parent  # e.g., .../group00041/\n",
        "        dir_name = img_dir.name  # e.g., \"00041\"\n",
        "        not_dir = parent_dir / f\"{dir_name}_not\"  # e.g., .../group00041/00041_not/\n",
        "        \n",
        "        txt_path = not_dir / f\"{base_id}.txt\"\n",
        "        \n",
        "        if not txt_path.exists():\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            # Get image dimensions\n",
        "            with Image.open(img_path) as img:\n",
        "                img_width, img_height = img.size\n",
        "            \n",
        "            # Process annotations\n",
        "            yolo_lines = []\n",
        "            with open(txt_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) < 5:\n",
        "                        continue\n",
        "                    \n",
        "                    # Deep PCB: x1 y1 x2 y2 class_id\n",
        "                    x1, y1, x2, y2, cls_id = map(int, parts[:5])\n",
        "                    \n",
        "                    if cls_id not in DEEPPCB_TO_YOLO:\n",
        "                        continue\n",
        "                    \n",
        "                    yolo_cls = DEEPPCB_TO_YOLO[cls_id]\n",
        "                    cx, cy, w, h = convert_bbox_to_yolo(\n",
        "                        img_width, img_height, x1, y1, x2, y2\n",
        "                    )\n",
        "                    \n",
        "                    yolo_lines.append(f\"{yolo_cls} {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\")\n",
        "            \n",
        "            if yolo_lines:\n",
        "                # Copy image to output directory\n",
        "                out_img_path = Path(img_out_dir) / img_path.name\n",
        "                if not out_img_path.exists():\n",
        "                    shutil.copy2(img_path, out_img_path)\n",
        "                \n",
        "                # Write YOLO label file\n",
        "                out_lbl_path = Path(lbl_out_dir) / (img_path.stem + '.txt')\n",
        "                with open(out_lbl_path, 'w') as f:\n",
        "                    f.write('\\n'.join(yolo_lines))\n",
        "                \n",
        "                processed += 1\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"  [WARN] Error processing {img_path.name}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return processed\n",
        "\n",
        "\n",
        "# Process the dataset\n",
        "print(\"Converting Deep PCB to YOLO format...\")\n",
        "num_processed = process_deeppcb_dataset(RAW_DIR, IMG_DIR, LBL_DIR)\n",
        "print(f\"\\n[OK] Processed {num_processed} images\")\n",
        "print(f\"   Images saved to: {IMG_DIR}\")\n",
        "print(f\"   Labels saved to: {LBL_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "load_dataset_to_snowflake_cell"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "# =============================================================================\n",
        "# LOAD DATA TO SNOWFLAKE TABLE\n",
        "# =============================================================================\n",
        "# Load processed images and labels into a Snowflake table for distributed access\n",
        "import base64\n",
        "\n",
        "def load_dataset_to_table(image_dir, label_dir, table_name=\"PCB_LABELED_DATA\"):\n",
        "    print(f\"Loading data from {image_dir} to table {table_name}...\")\n",
        "    \n",
        "    records = []\n",
        "    image_files = list(Path(image_dir).glob(\"*.jpg\"))\n",
        "    \n",
        "    for img_path in image_files:\n",
        "        # Read image as bytes\n",
        "        with open(img_path, \"rb\") as f:\n",
        "            img_bytes = f.read()\n",
        "            img_b64 = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
        "        \n",
        "        # Read label\n",
        "        lbl_path = Path(label_dir) / (img_path.stem + \".txt\")\n",
        "        label_text = \"\"\n",
        "        if lbl_path.exists():\n",
        "            with open(lbl_path, \"r\") as f:\n",
        "                label_text = f.read()\n",
        "        \n",
        "        # Determine split (hash based for consistency)\n",
        "        is_test = (hash(img_path.stem) % 10) < 2 # 20% test\n",
        "        split = \"TEST\" if is_test else \"TRAIN\"\n",
        "        \n",
        "        records.append({\n",
        "            \"FILENAME\": img_path.name,\n",
        "            \"IMAGE_BYTES\": img_b64,\n",
        "            \"LABEL_TEXT\": label_text,\n",
        "            \"SPLIT\": split\n",
        "        })\n",
        "        \n",
        "    if records:\n",
        "        df = pd.DataFrame(records)\n",
        "        session.write_pandas(\n",
        "            df, \n",
        "            table_name, \n",
        "            auto_create_table=True, \n",
        "            overwrite=True\n",
        "        )\n",
        "        \n",
        "        # =====================================================================\n",
        "        # FAIL-FAST: Verify row count after write\n",
        "        # =====================================================================\n",
        "        verify_df = session.sql(f\"SELECT COUNT(*) as CNT FROM {table_name}\").to_pandas()\n",
        "        if verify_df is None:\n",
        "            raise RuntimeError(f\"Verification query for {table_name} returned None\")\n",
        "        \n",
        "        row_count = int(verify_df['CNT'].iloc[0])\n",
        "        if row_count != len(records):\n",
        "            raise RuntimeError(\n",
        "                f\"Output verification failed: wrote {len(records)} rows, \"\n",
        "                f\"but table contains {row_count}\"\n",
        "            )\n",
        "        \n",
        "        print(f\"[OK] Loaded and verified {row_count} rows in {table_name}\")\n",
        "        \n",
        "        # Print split summary\n",
        "        train_count = sum(1 for r in records if r['SPLIT'] == 'TRAIN')\n",
        "        test_count = sum(1 for r in records if r['SPLIT'] == 'TEST')\n",
        "        print(f\"   Train: {train_count} images ({100*train_count/row_count:.1f}%)\")\n",
        "        print(f\"   Test:  {test_count} images ({100*test_count/row_count:.1f}%)\")\n",
        "    else:\n",
        "        raise RuntimeError(f\"No images found in {image_dir}. Data processing step may have failed.\")\n",
        "\n",
        "load_dataset_to_table(IMG_DIR, LBL_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f400ad28",
      "metadata": {
        "name": "yolov12_architecture_explainer"
      },
      "source": [
        "### What is YOLOv12?\n",
        "\n",
        "**YOLOv12** (You Only Look Once, version 12) is the latest iteration in the YOLO family of real-time object detection models. Released in early 2025, it introduces significant architectural improvements over previous versions.\n",
        "\n",
        "### Why YOLOv12 for PCB Defect Detection?\n",
        "\n",
        "1. **Real-time Performance**: YOLO models process entire images in a single forward pass, enabling real-time inspection speeds required for production lines\n",
        "2. **Small Object Detection**: PCB defects are often tiny (a few pixels); YOLOv12's improved feature pyramid handles multi-scale detection effectively\n",
        "3. **Single-Stage Detection**: Unlike two-stage detectors (R-CNN family), YOLO directly predicts bounding boxes and class probabilities, reducing latency\n",
        "4. **Transfer Learning**: Pre-trained weights from COCO dataset provide strong feature extractors that fine-tune well to industrial domains\n",
        "\n",
        "### Architecture Overview\n",
        "\n",
        "YOLOv12 builds on the CSPDarknet backbone with several key innovations:\n",
        "\n",
        "```\n",
        "Input Image (640x640)\n",
        "        â”‚\n",
        "        â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   CSPDarknet53    â”‚  â† Backbone: Feature extraction with cross-stage connections\n",
        "â”‚   + Area Attentionâ”‚  â† NEW: Efficient attention mechanism for global context\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "        â”‚\n",
        "        â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   PANet + SPPF    â”‚  â† Neck: Multi-scale feature fusion\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "        â”‚\n",
        "        â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Decoupled Head   â”‚  â† Head: Separate classification and regression branches\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "        â”‚\n",
        "        â–¼\n",
        "    [class, x, y, w, h, conf] Ã— N detections\n",
        "```\n",
        "\n",
        "### Key Hyperparameters\n",
        "\n",
        "| Parameter | Value | Purpose |\n",
        "|-----------|-------|---------|\n",
        "| `epochs` | 10 (demo) / 50+ (prod) | Training iterations over full dataset |\n",
        "| `imgsz` | 640 | Input resolution; higher = more detail but slower |\n",
        "| `batch` | 8 | Images per gradient update; limited by GPU memory |\n",
        "| `conf` | 0.25 | Confidence threshold for inference |\n",
        "\n",
        "### Loss Function\n",
        "\n",
        "YOLOv12 uses a composite loss combining:\n",
        "\n",
        "$$\\mathcal{L}_{total} = \\lambda_{box} \\mathcal{L}_{box} + \\lambda_{cls} \\mathcal{L}_{cls} + \\lambda_{dfl} \\mathcal{L}_{dfl}$$\n",
        "\n",
        "Where:\n",
        "- $\\mathcal{L}_{box}$: CIoU loss for bounding box regression\n",
        "- $\\mathcal{L}_{cls}$: Binary cross-entropy for class predictions  \n",
        "- $\\mathcal{L}_{dfl}$: Distribution focal loss for box refinement\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "execution_header"
      },
      "source": [
        "---\n",
        "## 6. Execution\n",
        "\n",
        "Train the YOLOv12 model on the converted Deep PCB dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "create_yolo_config"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CREATE YOLO DATA CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "data_config = {\n",
        "    'path': DATASET_DIR,\n",
        "    'train': 'images',  # Using same set for train/val in demo\n",
        "    'val': 'images',\n",
        "    'names': CLASS_NAMES\n",
        "}\n",
        "\n",
        "data_yaml_path = os.path.join(DATASET_DIR, 'data.yaml')\n",
        "with open(data_yaml_path, 'w') as f:\n",
        "    yaml.dump(data_config, f, default_flow_style=False)\n",
        "\n",
        "print(f\"[OK] Created data.yaml at {data_yaml_path}\")\n",
        "print(\"\\nConfiguration:\")\n",
        "print(yaml.dump(data_config, default_flow_style=False))\n",
        "\n",
        "# =============================================================================\n",
        "# DOWNLOAD PRETRAINED WEIGHTS FROM STAGE\n",
        "# =============================================================================\n",
        "# Download weights before distributed training (workers can't access GitHub)\n",
        "\n",
        "WEIGHTS_DIR = \"/tmp/weights\"\n",
        "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
        "\n",
        "print(\"\\nDownloading pretrained YOLO weights from stage...\")\n",
        "session.file.get(\"@MODEL_STAGE/weights/yolo12n.pt\", WEIGHTS_DIR)\n",
        "WEIGHTS_PATH = os.path.join(WEIGHTS_DIR, \"yolo12n.pt\")\n",
        "\n",
        "if os.path.exists(WEIGHTS_PATH):\n",
        "    print(f\"[OK] Weights downloaded to {WEIGHTS_PATH}\")\n",
        "else:\n",
        "    raise RuntimeError(\"Failed to download pretrained weights from @MODEL_STAGE/weights/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "training_diagnostics_utility"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DIAGNOSTICS UTILITY\n",
        "# =============================================================================\n",
        "import time\n",
        "class TrainingDiagnostics:\n",
        "    \"\"\"\n",
        "    Simple diagnostics collector for distributed training.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.epoch_times = []\n",
        "        self.start_time = None\n",
        "\n",
        "    def start_epoch(self):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def end_epoch(self, epoch, loss):\n",
        "        duration = time.time() - self.start_time\n",
        "        self.epoch_times.append(duration)\n",
        "        print(f\"Epoch {epoch}: Loss={loss:.4f}, Time={duration:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "train_yolov12_model"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DISTRIBUTED TRAINING\n",
        "# =============================================================================\n",
        "def train_func_yolo():\n",
        "    import os\n",
        "    import yaml\n",
        "    import shutil\n",
        "    import base64\n",
        "    import torch.distributed as dist\n",
        "    from ultralytics import YOLO\n",
        "    from snowflake.ml.modeling.distributors.pytorch import get_context\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # 1. Initialize distributed context FIRST (required for get_shard to work)\n",
        "    # -------------------------------------------------------------------------\n",
        "    context = get_context()\n",
        "    rank = context.get_rank()\n",
        "    dist.init_process_group(backend='nccl')\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # 2. Get data shard BEFORE clearing env vars (context detection needs them)\n",
        "    # -------------------------------------------------------------------------\n",
        "    dataset_map = context.get_dataset_map()\n",
        "    shard = dataset_map['train'].get_shard()\n",
        "    df = shard.to_pandas()\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # 3. NOW clear DDP env vars so YOLO uses single-GPU mode per worker\n",
        "    # -------------------------------------------------------------------------\n",
        "    for var in ['RANK', 'LOCAL_RANK', 'WORLD_SIZE', 'MASTER_ADDR', 'MASTER_PORT']:\n",
        "        os.environ.pop(var, None)\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # 4. Setup directories and materialize data to disk for YOLO\n",
        "    # -------------------------------------------------------------------------\n",
        "    WORKER_ROOT = f\"/tmp/worker_{rank}\"\n",
        "    IMG_DIR = f\"{WORKER_ROOT}/images\"\n",
        "    LBL_DIR = f\"{WORKER_ROOT}/labels\"\n",
        "    os.makedirs(IMG_DIR, exist_ok=True)\n",
        "    os.makedirs(LBL_DIR, exist_ok=True)\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        with open(f\"{IMG_DIR}/{row['FILENAME']}\", \"wb\") as f:\n",
        "            f.write(base64.b64decode(row['IMAGE_BYTES']))\n",
        "        lbl_name = row['FILENAME'].rsplit('.', 1)[0] + \".txt\"\n",
        "        with open(f\"{LBL_DIR}/{lbl_name}\", \"w\") as f:\n",
        "            f.write(row['LABEL_TEXT'])\n",
        "            \n",
        "    # -------------------------------------------------------------------------\n",
        "    # 5. Create YOLO data config\n",
        "    # -------------------------------------------------------------------------\n",
        "    data_config = {\n",
        "        'path': WORKER_ROOT,\n",
        "        'train': 'images',\n",
        "        'val': 'images',\n",
        "        'names': {0: 'open', 1: 'short', 2: 'mousebite', 3: 'spur', 4: 'copper', 5: 'pin-hole'}\n",
        "    }\n",
        "    yaml_path = f\"{WORKER_ROOT}/data.yaml\"\n",
        "    with open(yaml_path, 'w') as f:\n",
        "        yaml.dump(data_config, f)\n",
        "        \n",
        "    # -------------------------------------------------------------------------\n",
        "    # 6. Train YOLO model (using pre-downloaded weights from stage)\n",
        "    # -------------------------------------------------------------------------\n",
        "    model = YOLO(\"/tmp/weights/yolo12n.pt\")\n",
        "    results = model.train(\n",
        "        data=yaml_path,\n",
        "        epochs=10,\n",
        "        imgsz=640,\n",
        "        batch=16,\n",
        "        project=f\"{WORKER_ROOT}/runs\",\n",
        "        name=\"exp\",\n",
        "        device=0\n",
        "    )\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # 7. Save model artifact using context.get_model_dir() for proper sync\n",
        "    # -------------------------------------------------------------------------\n",
        "    if rank == 0:\n",
        "        src = f\"{WORKER_ROOT}/runs/exp/weights/best.pt\"\n",
        "        if os.path.exists(src):\n",
        "            # Save to context model dir (syncs to stage for multi-node)\n",
        "            model_dir = context.get_model_dir()\n",
        "            os.makedirs(model_dir, exist_ok=True)\n",
        "            shutil.copy2(src, os.path.join(model_dir, \"yolo_best.pt\"))\n",
        "            \n",
        "            # Also copy to /tmp/models for backward compatibility\n",
        "            os.makedirs(\"/tmp/models\", exist_ok=True)\n",
        "            shutil.copy2(src, \"/tmp/models/yolo_best.pt\")\n",
        "            \n",
        "    return results\n",
        "\n",
        "# Launch\n",
        "train_df = session.table(\"PCB_LABELED_DATA\").filter(col(\"SPLIT\") == \"TRAIN\")\n",
        "distributor = PyTorchDistributor(\n",
        "    train_func=train_func_yolo,\n",
        "    scaling_config=PyTorchScalingConfig(\n",
        "        num_nodes=1,\n",
        "        num_workers_per_node=1,\n",
        "        resource_requirements_per_worker=WorkerResourceConfig(num_cpus=0, num_gpus=1),\n",
        "    )\n",
        ")\n",
        "distributor.run(dataset_map={\"train\": ShardedDataConnector.from_dataframe(train_df)})\n",
        "print(\"Training Complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "evaluation_header"
      },
      "source": [
        "---\n",
        "## 7. Evaluation\n",
        "\n",
        "Analyze training results and visualize model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "training_results_visualization"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZE TRAINING RESULTS\n",
        "# =============================================================================\n",
        "\n",
        "# Define paths to training outputs (from distributed training worker 0)\n",
        "PROJECT_DIR = \"/tmp/worker_0/runs\"\n",
        "EXPERIMENT_NAME = \"exp\"\n",
        "\n",
        "# Load training results CSV\n",
        "results_csv = os.path.join(PROJECT_DIR, EXPERIMENT_NAME, 'results.csv')\n",
        "\n",
        "if os.path.exists(results_csv):\n",
        "    results_df = pd.read_csv(results_csv)\n",
        "    results_df.columns = results_df.columns.str.strip()  # Clean column names\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Plot 1: Training Loss\n",
        "    if 'train/box_loss' in results_df.columns:\n",
        "        axes[0].plot(results_df['epoch'], results_df['train/box_loss'], \n",
        "                     label='Box Loss', color=COLORS[0], linewidth=2)\n",
        "    if 'train/cls_loss' in results_df.columns:\n",
        "        axes[0].plot(results_df['epoch'], results_df['train/cls_loss'], \n",
        "                     label='Class Loss', color=COLORS[1], linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title('Training Loss Curves', fontweight='bold')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(alpha=0.3)\n",
        "    \n",
        "    # Plot 2: mAP Metrics\n",
        "    if 'metrics/mAP50(B)' in results_df.columns:\n",
        "        axes[1].plot(results_df['epoch'], results_df['metrics/mAP50(B)'], \n",
        "                     label='mAP@50', color=COLORS[2], linewidth=2)\n",
        "    if 'metrics/mAP50-95(B)' in results_df.columns:\n",
        "        axes[1].plot(results_df['epoch'], results_df['metrics/mAP50-95(B)'], \n",
        "                     label='mAP@50-95', color=COLORS[3], linewidth=2)\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('mAP')\n",
        "    axes[1].set_title('Validation mAP Metrics', fontweight='bold')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(alpha=0.3)\n",
        "    axes[1].set_ylim(0, 1)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/tmp/training_curves.png', dpi=150, bbox_inches='tight', facecolor='#121212')\n",
        "    plt.show()\n",
        "    \n",
        "    # Print final metrics\n",
        "    print(\"\\nFinal Training Metrics:\")\n",
        "    final_row = results_df.iloc[-1]\n",
        "    for col in results_df.columns:\n",
        "        if 'mAP' in col or 'precision' in col or 'recall' in col:\n",
        "            print(f\"  {col}: {final_row[col]:.4f}\")\n",
        "    \n",
        "    # =============================================================================\n",
        "    # AUTOMATED CONVERGENCE INTERPRETATION\n",
        "    # =============================================================================\n",
        "    # Analyze training dynamics to provide actionable recommendations\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CONVERGENCE ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Get final mAP50 for interpretation\n",
        "    final_map50 = final_row.get('metrics/mAP50(B)', 0)\n",
        "    final_map50_95 = final_row.get('metrics/mAP50-95(B)', 0)\n",
        "    \n",
        "    # Analyze loss trends (is it still decreasing?)\n",
        "    if 'train/box_loss' in results_df.columns and len(results_df) >= 3:\n",
        "        recent_losses = results_df['train/box_loss'].tail(3).values\n",
        "        loss_trend = recent_losses[-1] - recent_losses[0]\n",
        "        loss_converging = loss_trend < 0\n",
        "    else:\n",
        "        loss_converging = True  # Assume converging if can't determine\n",
        "    \n",
        "    # Determine overall assessment\n",
        "    if final_map50 >= 0.7:\n",
        "        status = \"[GOOD]\"\n",
        "        message = \"Model has converged well with strong detection performance.\"\n",
        "        recommendation = \"Ready for evaluation on held-out test data.\"\n",
        "    elif final_map50 >= 0.5:\n",
        "        status = \"[ACCEPTABLE]\"\n",
        "        message = \"Model shows reasonable performance but may benefit from more training.\"\n",
        "        if loss_converging:\n",
        "            recommendation = \"Consider increasing epochs to 30-50 for improved convergence.\"\n",
        "        else:\n",
        "            recommendation = \"Loss may have plateaued. Try adjusting learning rate or augmentation.\"\n",
        "    elif final_map50 >= 0.3:\n",
        "        status = \"[NEEDS IMPROVEMENT]\"\n",
        "        message = \"Detection performance is below target. Model may be underfitting.\"\n",
        "        recommendation = \"Increase training epochs to 50+, or check data quality and augmentation.\"\n",
        "    else:\n",
        "        status = \"[POOR]\"\n",
        "        message = \"Model is not learning effectively.\"\n",
        "        recommendation = \"Check: 1) Data loading, 2) Label format, 3) Learning rate, 4) Batch size.\"\n",
        "    \n",
        "    print(f\"\\n  Status: {status}\")\n",
        "    print(f\"  Assessment: {message}\")\n",
        "    print(f\"  Recommendation: {recommendation}\")\n",
        "    \n",
        "    # Additional insights\n",
        "    print(\"\\nPerformance Thresholds (for PCB defect detection):\")\n",
        "    print(\"  - Production-ready: mAP50 > 0.80\")\n",
        "    print(\"  - Acceptable: mAP50 > 0.60\")\n",
        "    print(\"  - Needs work: mAP50 < 0.50\")\n",
        "    print(f\"\\n  Current mAP@50: {final_map50:.3f}\")\n",
        "    print(f\"  Current mAP@50-95: {final_map50_95:.3f}\")\n",
        "    \n",
        "else:\n",
        "    print(\"[WARN] Training results CSV not found - training may not have completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "perclass_performance_visualization"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PER-CLASS PERFORMANCE BREAKDOWN\n",
        "# =============================================================================\n",
        "# Identify which defect classes are easier/harder to detect.\n",
        "# This informs targeted data collection or augmentation strategies.\n",
        "\n",
        "# Check if validation results are available\n",
        "val_results_path = os.path.join(PROJECT_DIR, EXPERIMENT_NAME, 'results.csv')\n",
        "\n",
        "if os.path.exists(val_results_path):\n",
        "    # Load metrics CSV and look for per-class metrics\n",
        "    metrics_df = pd.read_csv(val_results_path)\n",
        "    metrics_df.columns = metrics_df.columns.str.strip()\n",
        "    \n",
        "    # YOLO stores per-class metrics in the final row or in separate files\n",
        "    # Check for per-class columns or use the trained model to evaluate\n",
        "    \n",
        "    # If we have the trained model, we can get per-class metrics\n",
        "    best_model_path = os.path.join(PROJECT_DIR, EXPERIMENT_NAME, 'weights', 'best.pt')\n",
        "    \n",
        "    if os.path.exists(best_model_path):\n",
        "        print(\"Calculating per-class detection statistics...\")\n",
        "        \n",
        "        eval_model = YOLO(best_model_path)\n",
        "        \n",
        "        # Run inference on all images and collect per-class stats\n",
        "        class_stats = {name: {'tp': 0, 'fp': 0, 'fn': 0, 'conf_sum': 0, 'count': 0} \n",
        "                       for name in CLASS_NAMES.values()}\n",
        "        \n",
        "        test_images = list(Path(IMG_DIR).glob('*.jpg'))[:50]  # Sample for speed\n",
        "        \n",
        "        for img_path in test_images:\n",
        "            results = eval_model.predict(str(img_path), conf=0.25, verbose=False)\n",
        "            \n",
        "            for box in results[0].boxes:\n",
        "                cls_id = int(box.cls[0])\n",
        "                conf = float(box.conf[0])\n",
        "                class_name = CLASS_NAMES.get(cls_id, 'unknown')\n",
        "                \n",
        "                if class_name in class_stats:\n",
        "                    class_stats[class_name]['count'] += 1\n",
        "                    class_stats[class_name]['conf_sum'] += conf\n",
        "        \n",
        "        # Calculate average confidence per class\n",
        "        class_names_list = []\n",
        "        detection_counts = []\n",
        "        avg_confidences = []\n",
        "        \n",
        "        for name, stats in class_stats.items():\n",
        "            class_names_list.append(name)\n",
        "            detection_counts.append(stats['count'])\n",
        "            avg_conf = stats['conf_sum'] / stats['count'] if stats['count'] > 0 else 0\n",
        "            avg_confidences.append(avg_conf)\n",
        "        \n",
        "        # Create visualization\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        \n",
        "        # Plot 1: Detection counts by class\n",
        "        bars1 = axes[0].bar(class_names_list, detection_counts, color=COLORS[:len(class_names_list)],\n",
        "                           edgecolor='white', linewidth=0.5)\n",
        "        axes[0].set_xlabel('Defect Class')\n",
        "        axes[0].set_ylabel('Number of Detections')\n",
        "        axes[0].set_title('Detections by Class (Sample of 50 Images)', fontweight='bold')\n",
        "        axes[0].grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        # Add value labels\n",
        "        for bar, count in zip(bars1, detection_counts):\n",
        "            if count > 0:\n",
        "                axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                            str(count), ha='center', va='bottom', fontsize=10, color='#E5E5E7')\n",
        "        \n",
        "        # Plot 2: Average confidence by class\n",
        "        bars2 = axes[1].bar(class_names_list, avg_confidences, color=COLORS[:len(class_names_list)],\n",
        "                           edgecolor='white', linewidth=0.5)\n",
        "        axes[1].set_xlabel('Defect Class')\n",
        "        axes[1].set_ylabel('Average Confidence')\n",
        "        axes[1].set_title('Average Detection Confidence by Class', fontweight='bold')\n",
        "        axes[1].set_ylim(0, 1)\n",
        "        axes[1].axhline(y=0.5, color='#FF6B6B', linestyle='--', alpha=0.7, label='Threshold')\n",
        "        axes[1].grid(axis='y', alpha=0.3)\n",
        "        axes[1].legend()\n",
        "        \n",
        "        # Add value labels\n",
        "        for bar, conf in zip(bars2, avg_confidences):\n",
        "            if conf > 0:\n",
        "                axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "                            f'{conf:.2f}', ha='center', va='bottom', fontsize=10, color='#E5E5E7')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/tmp/perclass_performance.png', dpi=150, bbox_inches='tight', facecolor='#121212')\n",
        "        plt.show()\n",
        "        \n",
        "        # Print insights\n",
        "        print(\"\\nPer-Class Analysis:\")\n",
        "        sorted_classes = sorted(zip(class_names_list, detection_counts, avg_confidences), \n",
        "                               key=lambda x: x[1], reverse=True)\n",
        "        for name, count, conf in sorted_classes:\n",
        "            status = \"[GOOD]\" if conf >= 0.6 else (\"[WARN]\" if conf >= 0.4 else \"[POOR]\")\n",
        "            print(f\"  {status} {name}: {count} detections, avg conf: {conf:.3f}\")\n",
        "        \n",
        "        # Identify classes needing attention\n",
        "        low_conf_classes = [name for name, _, conf in sorted_classes if conf < 0.5 and conf > 0]\n",
        "        if low_conf_classes:\n",
        "            print(f\"\\n[WARN] Classes with low confidence (< 0.5): {', '.join(low_conf_classes)}\")\n",
        "            print(\"   Consider: More training data or targeted augmentation for these classes.\")\n",
        "    else:\n",
        "        print(\"[WARN] Best model not found for per-class evaluation\")\n",
        "else:\n",
        "    print(\"[WARN] Validation results not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "sample_predictions_visualization"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZE SAMPLE PREDICTIONS\n",
        "# =============================================================================\n",
        "\n",
        "# Load best model\n",
        "best_model_path = os.path.join(PROJECT_DIR, EXPERIMENT_NAME, 'weights', 'best.pt')\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    best_model = YOLO(best_model_path)\n",
        "    \n",
        "    # Get sample images for prediction\n",
        "    sample_images = list(Path(IMG_DIR).glob('*.jpg'))[:4]\n",
        "    \n",
        "    if sample_images:\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
        "        axes = axes.flatten()\n",
        "        \n",
        "        for idx, img_path in enumerate(sample_images):\n",
        "            # Run inference - returns list of Results objects (one per image)\n",
        "            results = best_model.predict(str(img_path), conf=0.25, verbose=False)\n",
        "            # results[0]: Results object for first (only) image\n",
        "            # results[0].boxes: Boxes object, shape (N, 6) for N detections\n",
        "            # results[0].orig_shape: tuple (H, W) of original image\n",
        "            \n",
        "            # Plot with bounding boxes - returns numpy array (H, W, 3) BGR\n",
        "            annotated = results[0].plot()\n",
        "            \n",
        "            # Convert BGR (OpenCV format) to RGB (matplotlib format)\n",
        "            # annotated shape: (H, W, 3) -> reverse last axis for RGB\n",
        "            annotated_rgb = annotated[:, :, ::-1]\n",
        "            \n",
        "            axes[idx].imshow(annotated_rgb)\n",
        "            axes[idx].set_title(f'Sample {idx+1}: {img_path.name}', fontsize=10)\n",
        "            axes[idx].axis('off')\n",
        "            \n",
        "            # Count detections\n",
        "            num_detections = len(results[0].boxes)\n",
        "            axes[idx].text(10, 30, f'Detections: {num_detections}', \n",
        "                          color='white', fontsize=10,\n",
        "                          bbox=dict(boxstyle='round', facecolor='#121212', alpha=0.8))\n",
        "        \n",
        "        plt.suptitle('YOLOv12 PCB Defect Detection Results', fontsize=14, fontweight='bold', y=1.02)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/tmp/sample_predictions.png', dpi=150, bbox_inches='tight', facecolor='#121212')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"[WARN] No sample images found for prediction\")\n",
        "else:\n",
        "    print(f\"[WARN] Best model not found at {best_model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "confidence_distribution_visualization"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIDENCE SCORE DISTRIBUTION\n",
        "# =============================================================================\n",
        "# Analyze the distribution of prediction confidences to understand model\n",
        "# certainty and inform threshold selection for production deployment.\n",
        "\n",
        "best_model_path = os.path.join(PROJECT_DIR, EXPERIMENT_NAME, 'weights', 'best.pt')\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    conf_model = YOLO(best_model_path)\n",
        "    \n",
        "    # Collect all confidence scores\n",
        "    all_confidences = []\n",
        "    confidence_by_class = {name: [] for name in CLASS_NAMES.values()}\n",
        "    \n",
        "    test_images = list(Path(IMG_DIR).glob('*.jpg'))\n",
        "    \n",
        "    for img_path in test_images:\n",
        "        results = conf_model.predict(str(img_path), conf=0.1, verbose=False)  # Lower threshold to see distribution\n",
        "        \n",
        "        for box in results[0].boxes:\n",
        "            cls_id = int(box.cls[0])\n",
        "            conf = float(box.conf[0])\n",
        "            all_confidences.append(conf)\n",
        "            \n",
        "            class_name = CLASS_NAMES.get(cls_id, 'unknown')\n",
        "            if class_name in confidence_by_class:\n",
        "                confidence_by_class[class_name].append(conf)\n",
        "    \n",
        "    if all_confidences:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        \n",
        "        # Plot 1: Overall confidence distribution\n",
        "        axes[0].hist(all_confidences, bins=50, color=COLORS[0], edgecolor='white', \n",
        "                    linewidth=0.5, alpha=0.8)\n",
        "        \n",
        "        # Add threshold markers\n",
        "        thresholds = [(0.25, 'Default (0.25)', '#FF6B6B'),\n",
        "                      (0.5, 'Medium (0.5)', '#FFD60A'),\n",
        "                      (0.8, 'High (0.8)', '#30D158')]\n",
        "        \n",
        "        for thresh, label, color in thresholds:\n",
        "            axes[0].axvline(x=thresh, color=color, linestyle='--', linewidth=2, label=label)\n",
        "        \n",
        "        axes[0].set_xlabel('Confidence Score')\n",
        "        axes[0].set_ylabel('Number of Detections')\n",
        "        axes[0].set_title('Overall Confidence Distribution', fontweight='bold')\n",
        "        axes[0].legend(loc='upper right')\n",
        "        axes[0].grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        # Plot 2: Box plot by class\n",
        "        box_data = [confidence_by_class[name] for name in CLASS_NAMES.values() if confidence_by_class[name]]\n",
        "        box_labels = [name for name in CLASS_NAMES.values() if confidence_by_class[name]]\n",
        "        \n",
        "        if box_data:\n",
        "            bp = axes[1].boxplot(box_data, labels=box_labels, patch_artist=True)\n",
        "            \n",
        "            # Color the boxes\n",
        "            for patch, color in zip(bp['boxes'], COLORS[:len(box_data)]):\n",
        "                patch.set_facecolor(color)\n",
        "                patch.set_alpha(0.7)\n",
        "            \n",
        "            axes[1].axhline(y=0.5, color='#FF6B6B', linestyle='--', alpha=0.7, label='Threshold')\n",
        "            axes[1].set_xlabel('Defect Class')\n",
        "            axes[1].set_ylabel('Confidence Score')\n",
        "            axes[1].set_title('Confidence Distribution by Class', fontweight='bold')\n",
        "            axes[1].set_ylim(0, 1)\n",
        "            axes[1].grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('/tmp/confidence_distribution.png', dpi=150, bbox_inches='tight', facecolor='#121212')\n",
        "        plt.show()\n",
        "        \n",
        "        # Print statistics\n",
        "        print(\"\\nConfidence Score Statistics:\")\n",
        "        print(f\"  Total detections (conf > 0.1): {len(all_confidences)}\")\n",
        "        print(f\"  Mean confidence: {np.mean(all_confidences):.3f}\")\n",
        "        print(f\"  Median confidence: {np.median(all_confidences):.3f}\")\n",
        "        print(f\"  Std deviation: {np.std(all_confidences):.3f}\")\n",
        "        \n",
        "        # Threshold analysis\n",
        "        print(\"\\nDetections at Different Thresholds:\")\n",
        "        for thresh in [0.25, 0.5, 0.75, 0.9]:\n",
        "            count = sum(1 for c in all_confidences if c >= thresh)\n",
        "            pct = 100 * count / len(all_confidences)\n",
        "            print(f\"  conf >= {thresh}: {count} detections ({pct:.1f}%)\")\n",
        "        \n",
        "        # Recommendation\n",
        "        high_conf_pct = sum(1 for c in all_confidences if c >= 0.7) / len(all_confidences) * 100\n",
        "        if high_conf_pct > 60:\n",
        "            print(\"\\nRecommendation: Model shows high confidence. Safe to use threshold 0.5-0.7.\")\n",
        "        elif high_conf_pct > 30:\n",
        "            print(\"\\n[WARN] Recommendation: Mixed confidence. Consider threshold 0.3-0.5 with manual review.\")\n",
        "        else:\n",
        "            print(\"\\n[WARN] Recommendation: Low overall confidence. Review model or increase training epochs.\")\n",
        "    else:\n",
        "        print(\"[WARN] No detections found for confidence analysis\")\n",
        "else:\n",
        "    print(\"[WARN] Best model not found for confidence analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "production_output_header"
      },
      "source": [
        "---\n",
        "## 8. Production Output\n",
        "\n",
        "Log the trained model to Snowflake's Model Registry and write inference results to the DEFECT_LOGS table for dashboard visualization.\n",
        "\n",
        "**Model Registry Benefits:**\n",
        "- Version control for model iterations\n",
        "- SQL-based inference via `MODEL!predict()` syntax\n",
        "- Deployment to Snowpark Container Services for GPU inference\n",
        "- Built-in model metadata and lineage tracking\n",
        "\n",
        "**Output:**\n",
        "- Model logged to registry as `YOLO_PCB_DEFECT_DETECTOR` version `v1`\n",
        "- Model also saved to `@MODEL_STAGE/models/yolov12_pcb/yolo_best.pt` for Vision Lab compatibility\n",
        "- Inference results written to `DEFECT_LOGS` table (enables Streamlit dashboard)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "define_custom_model_class"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DEFINE CUSTOM MODEL CLASS FOR MODEL REGISTRY\n",
        "# =============================================================================\n",
        "# Wrap the YOLO model in a CustomModel class to enable logging to Snowflake's\n",
        "# Model Registry. This allows versioning, SQL-based inference, and SPCS deployment.\n",
        "\n",
        "from snowflake.ml.model import custom_model\n",
        "from snowflake.ml.model import model_signature\n",
        "import base64\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "# Class name mapping (must match training)\n",
        "CLASS_NAMES_MAP = {\n",
        "    0: 'open',\n",
        "    1: 'short', \n",
        "    2: 'mousebite',\n",
        "    3: 'spur',\n",
        "    4: 'copper',\n",
        "    5: 'pin-hole'\n",
        "}\n",
        "\n",
        "class YOLOPCBModel(custom_model.CustomModel):\n",
        "    \"\"\"\n",
        "    Custom model wrapper for YOLOv12 PCB defect detection.\n",
        "    \n",
        "    This wrapper enables logging the YOLO model to Snowflake's Model Registry,\n",
        "    which provides versioning, SQL inference, and SPCS deployment capabilities.\n",
        "    \n",
        "    Input: DataFrame with IMAGE_BYTES column (base64-encoded image data)\n",
        "    Output: DataFrame with detection results (class, confidence, bbox coordinates)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, context: custom_model.ModelContext) -> None:\n",
        "        super().__init__(context)\n",
        "        from ultralytics import YOLO\n",
        "        # Load YOLO model from the file provided in context\n",
        "        self.model = YOLO(self.context[\"model_file\"])\n",
        "    \n",
        "    @custom_model.inference_api\n",
        "    def predict(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Run inference on base64-encoded images.\n",
        "        \n",
        "        Args:\n",
        "            input_df: DataFrame with columns:\n",
        "                - IMAGE_BYTES: base64-encoded image data (string)\n",
        "                - FILENAME: optional, image filename for tracking\n",
        "        \n",
        "        Returns:\n",
        "            DataFrame with columns:\n",
        "                - FILENAME: source image filename\n",
        "                - DETECTED_CLASS: defect class name\n",
        "                - CONFIDENCE: detection confidence score\n",
        "                - BBOX_X_CENTER: normalized bbox center x\n",
        "                - BBOX_Y_CENTER: normalized bbox center y\n",
        "                - BBOX_WIDTH: normalized bbox width\n",
        "                - BBOX_HEIGHT: normalized bbox height\n",
        "        \"\"\"\n",
        "        from PIL import Image\n",
        "        \n",
        "        results_list = []\n",
        "        \n",
        "        for idx, row in input_df.iterrows():\n",
        "            # Decode base64 image\n",
        "            img_bytes = base64.b64decode(row['IMAGE_BYTES'])\n",
        "            img = Image.open(io.BytesIO(img_bytes))\n",
        "            \n",
        "            # Get filename if available\n",
        "            filename = row.get('FILENAME', f'image_{idx}')\n",
        "            \n",
        "            # Run YOLO inference\n",
        "            detections = self.model.predict(img, conf=0.25, verbose=False)\n",
        "            \n",
        "            # Process each detection\n",
        "            for box in detections[0].boxes:\n",
        "                cls_id = int(box.cls[0])\n",
        "                confidence = float(box.conf[0])\n",
        "                bbox = box.xywhn[0].tolist()  # Normalized coordinates\n",
        "                \n",
        "                results_list.append({\n",
        "                    'FILENAME': filename,\n",
        "                    'DETECTED_CLASS': CLASS_NAMES_MAP.get(cls_id, 'unknown'),\n",
        "                    'CONFIDENCE': confidence,\n",
        "                    'BBOX_X_CENTER': bbox[0],\n",
        "                    'BBOX_Y_CENTER': bbox[1],\n",
        "                    'BBOX_WIDTH': bbox[2],\n",
        "                    'BBOX_HEIGHT': bbox[3]\n",
        "                })\n",
        "        \n",
        "        # Return empty DataFrame with correct schema if no detections\n",
        "        if not results_list:\n",
        "            return pd.DataFrame(columns=[\n",
        "                'FILENAME', 'DETECTED_CLASS', 'CONFIDENCE',\n",
        "                'BBOX_X_CENTER', 'BBOX_Y_CENTER', 'BBOX_WIDTH', 'BBOX_HEIGHT'\n",
        "            ])\n",
        "        \n",
        "        return pd.DataFrame(results_list)\n",
        "\n",
        "print(\"[OK] YOLOPCBModel class defined for Model Registry\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "persist_model_to_stage"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LOG MODEL TO SNOWFLAKE MODEL REGISTRY\n",
        "# =============================================================================\n",
        "# Log the trained YOLO model to Snowflake's Model Registry using CustomModel.\n",
        "# This enables model versioning, SQL-based inference, and SPCS deployment.\n",
        "\n",
        "from snowflake.ml.model._signatures import snowpark_handler\n",
        "from snowflake.ml.model.model_signature import FeatureSpec, DataType, ModelSignature\n",
        "\n",
        "model_path = \"/tmp/models/yolo_best.pt\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 1. Define Model Signature (input/output schema)\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Input: DataFrame with IMAGE_BYTES (base64 string) and optional FILENAME\n",
        "    input_features = [\n",
        "        FeatureSpec(name=\"IMAGE_BYTES\", dtype=DataType.STRING),\n",
        "        FeatureSpec(name=\"FILENAME\", dtype=DataType.STRING),\n",
        "    ]\n",
        "    \n",
        "    # Output: DataFrame with detection results\n",
        "    output_features = [\n",
        "        FeatureSpec(name=\"FILENAME\", dtype=DataType.STRING),\n",
        "        FeatureSpec(name=\"DETECTED_CLASS\", dtype=DataType.STRING),\n",
        "        FeatureSpec(name=\"CONFIDENCE\", dtype=DataType.DOUBLE),\n",
        "        FeatureSpec(name=\"BBOX_X_CENTER\", dtype=DataType.DOUBLE),\n",
        "        FeatureSpec(name=\"BBOX_Y_CENTER\", dtype=DataType.DOUBLE),\n",
        "        FeatureSpec(name=\"BBOX_WIDTH\", dtype=DataType.DOUBLE),\n",
        "        FeatureSpec(name=\"BBOX_HEIGHT\", dtype=DataType.DOUBLE),\n",
        "    ]\n",
        "    \n",
        "    predict_signature = ModelSignature(inputs=input_features, outputs=output_features)\n",
        "    print(\"[OK] Model signature defined\")\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # 2. Create Model Context with the trained weights file\n",
        "    # -------------------------------------------------------------------------\n",
        "    model_context = custom_model.ModelContext(model_file=model_path)\n",
        "    yolo_custom_model = YOLOPCBModel(model_context)\n",
        "    print(\"[OK] Custom model instance created\")\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # 3. Log to Model Registry\n",
        "    # -------------------------------------------------------------------------\n",
        "    reg = Registry(\n",
        "        session=session,\n",
        "        database_name=session.get_current_database(),\n",
        "        schema_name=session.get_current_schema()\n",
        "    )\n",
        "    \n",
        "    MODEL_NAME = \"YOLO_PCB_DEFECT_DETECTOR\"\n",
        "    VERSION_NAME = \"v1\"\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # 3a. Clean up existing version for idempotent execution\n",
        "    # -------------------------------------------------------------------------\n",
        "    try:\n",
        "        existing_model = reg.get_model(MODEL_NAME)\n",
        "        versions = [v.version_name for v in existing_model.versions()]\n",
        "        \n",
        "        if VERSION_NAME in versions:\n",
        "            if len(versions) == 1:\n",
        "                # Only version - delete entire model\n",
        "                reg.delete_model(MODEL_NAME)\n",
        "                print(f\"[DELETED] Existing model {MODEL_NAME} (was only version)\")\n",
        "            else:\n",
        "                # Multiple versions - just delete this version\n",
        "                existing_model.delete_version(VERSION_NAME)\n",
        "                print(f\"[DELETED] Existing version {VERSION_NAME}\")\n",
        "    except Exception:\n",
        "        # Model doesn't exist yet - this is expected on first run\n",
        "        print(f\"[INFO] No existing model {MODEL_NAME} found (first run)\")\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # 3b. Log the model with SPCS as target platform (GPU inference)\n",
        "    # -------------------------------------------------------------------------\n",
        "    mv = reg.log_model(\n",
        "        yolo_custom_model,\n",
        "        model_name=MODEL_NAME,\n",
        "        version_name=VERSION_NAME,\n",
        "        pip_requirements=[\"ultralytics>=8.0.0\", \"opencv-python-headless\", \"pillow\"],\n",
        "        target_platforms=[\"SNOWPARK_CONTAINER_SERVICES\"],\n",
        "        signatures={\"predict\": predict_signature},\n",
        "        comment=\"YOLOv12 PCB defect detection model trained on Deep PCB dataset (6 classes)\"\n",
        "    )\n",
        "    \n",
        "    print(f\"[OK] Model logged to registry: {mv.model_name} version {mv.version_name}\")\n",
        "    print(f\"ðŸ“¦ Model methods: {mv.show_functions()}\")\n",
        "    \n",
        "    # Also upload to stage for backward compatibility with Vision Lab\n",
        "    stage_path = \"@MODEL_STAGE/models/yolov12_pcb/\"\n",
        "    session.file.put(model_path, stage_path, auto_compress=False, overwrite=True)\n",
        "    print(f\"ðŸ“ Also uploaded to stage for Vision Lab: {stage_path}yolo_best.pt\")\n",
        "    \n",
        "else:\n",
        "    print(\"[WARN] Model file not found at /tmp/models/yolo_best.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "test_model_registry_inference"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TEST MODEL REGISTRY INFERENCE\n",
        "# =============================================================================\n",
        "# Verify the logged model works by running inference through the registry.\n",
        "\n",
        "# Get a few test images from the processed dataset\n",
        "test_images = list(Path(IMG_DIR).glob('*.jpg'))[:3]\n",
        "\n",
        "if test_images and 'mv' in dir():\n",
        "    print(\"Testing Model Registry inference...\")\n",
        "    \n",
        "    # Prepare test data in the expected format\n",
        "    test_records = []\n",
        "    for img_path in test_images:\n",
        "        with open(img_path, \"rb\") as f:\n",
        "            img_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "        test_records.append({\n",
        "            \"IMAGE_BYTES\": img_b64,\n",
        "            \"FILENAME\": img_path.name\n",
        "        })\n",
        "    \n",
        "    test_df = pd.DataFrame(test_records)\n",
        "    print(f\"Test input shape: {test_df.shape}\")\n",
        "    \n",
        "    # Run inference through the model registry\n",
        "    # Note: This uses SPCS for inference (GPU-enabled)\n",
        "    try:\n",
        "        result_df = mv.run(test_df, function_name=\"predict\")\n",
        "        \n",
        "        print(f\"\\n[OK] Model Registry inference successful!\")\n",
        "        print(f\"Output shape: {result_df.shape}\")\n",
        "        print(\"\\nSample detections:\")\n",
        "        print(result_df.head(10).to_string(index=False))\n",
        "        \n",
        "        # Summary by class\n",
        "        if len(result_df) > 0:\n",
        "            class_summary = result_df.groupby('DETECTED_CLASS').agg({\n",
        "                'CONFIDENCE': ['count', 'mean']\n",
        "            }).round(3)\n",
        "            print(\"\\nDetection summary by class:\")\n",
        "            print(class_summary)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Registry inference test failed: {e}\")\n",
        "        print(\"   This may be expected if SPCS is not yet deployed.\")\n",
        "        print(\"   The model is logged and can be deployed for inference later.\")\n",
        "else:\n",
        "    if 'mv' not in dir():\n",
        "        print(\"[WARN] Model version (mv) not available - run the logging cell first\")\n",
        "    else:\n",
        "        print(\"[WARN] No test images found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "write_inference_results_to_table"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# WRITE INFERENCE RESULTS TO DEFECT_LOGS TABLE\n",
        "# =============================================================================\n",
        "# Run inference on all processed images and write detection results to\n",
        "# the DEFECT_LOGS table for dashboard visualization.\n",
        "\n",
        "# Load the trained model\n",
        "model_path = \"/tmp/models/yolo_best.pt\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    inference_model = YOLO(model_path)\n",
        "    \n",
        "    # Get all processed images\n",
        "    test_images = list(Path(IMG_DIR).glob('*.jpg'))\n",
        "    print(f\"Running inference on {len(test_images)} images...\")\n",
        "    \n",
        "    # Collect inference results\n",
        "    inference_records = []\n",
        "    \n",
        "    for img_path in test_images:\n",
        "        # Run inference\n",
        "        results = inference_model.predict(str(img_path), conf=0.25, verbose=False)\n",
        "        \n",
        "        # Generate a board_id from filename (e.g., \"00041001_temp.jpg\" -> \"PCB_00041001\")\n",
        "        img_stem = img_path.stem.replace('_temp', '').replace('_test', '')\n",
        "        board_id = f\"PCB_{img_stem}\"\n",
        "        \n",
        "        # Stage path for the image\n",
        "        image_stage_path = f\"@MODEL_STAGE/raw/deeppcb/{img_path.name}\"\n",
        "        \n",
        "        # Process each detection\n",
        "        for box in results[0].boxes:\n",
        "            cls_id = int(box.cls[0])\n",
        "            confidence = float(box.conf[0])\n",
        "            \n",
        "            # Get normalized bbox (already in YOLO format: x_center, y_center, w, h)\n",
        "            bbox = box.xywhn[0].tolist()  # Normalized coordinates\n",
        "            \n",
        "            inference_records.append({\n",
        "                'INFERENCE_ID': str(uuid.uuid4()),\n",
        "                'BOARD_ID': board_id,\n",
        "                'INFERENCE_TIMESTAMP': datetime.now(),\n",
        "                'DETECTED_CLASS': CLASS_NAMES.get(cls_id, 'unknown'),\n",
        "                'CONFIDENCE_SCORE': confidence,\n",
        "                'BBOX_X_CENTER': bbox[0],\n",
        "                'BBOX_Y_CENTER': bbox[1],\n",
        "                'BBOX_WIDTH': bbox[2],\n",
        "                'BBOX_HEIGHT': bbox[3],\n",
        "                'IMAGE_PATH': image_stage_path,\n",
        "                'MODEL_VERSION': 'yolov12n_pcb_v1'\n",
        "            })\n",
        "    \n",
        "    if inference_records:\n",
        "        # Write to DEFECT_LOGS table\n",
        "        inference_df = pd.DataFrame(inference_records)\n",
        "        session.write_pandas(\n",
        "            inference_df,\n",
        "            'DEFECT_LOGS',\n",
        "            auto_create_table=False,\n",
        "            overwrite=False  # Append to existing data\n",
        "        )\n",
        "        \n",
        "        print(f\"[OK] Wrote {len(inference_records)} detections to DEFECT_LOGS table\")\n",
        "        \n",
        "        # Show summary\n",
        "        summary = inference_df.groupby('DETECTED_CLASS').size()\n",
        "        print(\"\\nDetection Summary:\")\n",
        "        for cls, count in summary.items():\n",
        "            print(f\"   {cls}: {count}\")\n",
        "        \n",
        "        # Show sample of what was written\n",
        "        print(\"\\nSample records written:\")\n",
        "        print(inference_df[['DETECTED_CLASS', 'CONFIDENCE_SCORE', 'BOARD_ID']].head(10).to_string(index=False))\n",
        "    else:\n",
        "        print(\"[WARN] No detections found above confidence threshold\")\n",
        "else:\n",
        "    print(\"[WARN] Model not found - run training first\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "name": "model_deployment_options"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MODEL REGISTRY DEPLOYMENT OPTIONS\n",
        "# =============================================================================\n",
        "# The model has been logged to Snowflake's Model Registry with target platform \n",
        "# set to SNOWPARK_CONTAINER_SERVICES for GPU inference.\n",
        "#\n",
        "# DEPLOYMENT OPTIONS:\n",
        "#\n",
        "# 1. SPCS Model Serving (Recommended for production):\n",
        "#    Deploy the model to SPCS for GPU-accelerated inference:\n",
        "#    ```python\n",
        "#    mv.deploy(\n",
        "#        deployment_name=\"yolo_pcb_service\",\n",
        "#        target_method=\"predict\",\n",
        "#        compute_pool=\"GPU_POOL\",\n",
        "#        num_gpus=1\n",
        "#    )\n",
        "#    ```\n",
        "#\n",
        "# 2. SQL Inference (via SPCS):\n",
        "#    Once deployed, call from SQL:\n",
        "#    ```sql\n",
        "#    SELECT YOLO_PCB_DEFECT_DETECTOR!predict(IMAGE_BYTES, FILENAME) \n",
        "#    FROM my_images_table;\n",
        "#    ```\n",
        "#\n",
        "# 3. Python Inference (via Registry):\n",
        "#    Use mv.run() as demonstrated in the test cell above.\n",
        "#\n",
        "# MODEL LOCATIONS:\n",
        "# - Model Registry: YOLO_PCB_DEFECT_DETECTOR (version v1)\n",
        "# - Stage backup: @MODEL_STAGE/models/yolov12_pcb/yolo_best.pt\n",
        "\n",
        "print(\"[OK] Model logged to Snowflake Model Registry\")\n",
        "print(\"ðŸ“¦ Registry model: YOLO_PCB_DEFECT_DETECTOR v1\")\n",
        "print(\"ðŸ“ Stage backup: @MODEL_STAGE/models/yolov12_pcb/yolo_best.pt\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"   1. Deploy to SPCS with mv.deploy() for production inference\")\n",
        "print(\"   2. View model in Snowsight: AI & ML > Models\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "name": "key_takeaways_header"
      },
      "source": [
        "---\n",
        "## 9. Key Takeaways & Interpretation Guide\n",
        "\n",
        "### What the Model Learned\n",
        "\n",
        "The YOLOv12 model was trained to detect 6 classes of PCB manufacturing defects:\n",
        "\n",
        "1. **Open circuits**: Broken or missing conductive traces\n",
        "2. **Short circuits**: Unintended connections between traces\n",
        "3. **Mousebites**: Irregular edge erosion on copper features\n",
        "4. **Spurs**: Unwanted copper protrusions from traces\n",
        "5. **Copper defects**: Missing or excess copper areas\n",
        "6. **Pin-holes**: Small voids in copper layers\n",
        "\n",
        "The model learns to identify these defects by:\n",
        "- Extracting hierarchical visual features through the CSPDarknet backbone\n",
        "- Fusing multi-scale features via PANet for detecting both small and large defects\n",
        "- Predicting bounding boxes and class probabilities in a single forward pass\n",
        "\n",
        "### Interpretation Guidelines\n",
        "\n",
        "| Metric | Value Range | Interpretation |\n",
        "|--------|-------------|----------------|\n",
        "| **Confidence Score** | 0.0 - 1.0 | Model certainty; >0.7 = high confidence, 0.4-0.7 = review recommended, <0.4 = likely false positive |\n",
        "| **mAP@50** | 0.0 - 1.0 | Detection accuracy at 50% IoU threshold; >0.8 = production-ready, >0.6 = acceptable |\n",
        "| **mAP@50-95** | 0.0 - 1.0 | Stricter accuracy across IoU thresholds; typically 0.3-0.5 lower than mAP@50 |\n",
        "| **Box Loss** | Decreasing | Should decrease during training; plateau indicates convergence |\n",
        "| **Class Loss** | Decreasing | Classification accuracy improvement; should stabilize |\n",
        "\n",
        "### Limitations & Considerations\n",
        "\n",
        "1. **Demo Training Duration**: This notebook uses 10 epochs for demonstration. Production models should train for 50-100+ epochs for optimal performance.\n",
        "\n",
        "2. **Dataset Size**: Deep PCB is a relatively small dataset (~1,500 images). Performance may improve with larger, more diverse training data.\n",
        "\n",
        "3. **Class Imbalance**: Some defect classes (e.g., pin-hole) may have fewer training examples, leading to lower detection accuracy for rare defects.\n",
        "\n",
        "4. **Domain Shift**: Model trained on Deep PCB dataset may require fine-tuning when applied to PCBs with different designs, lighting conditions, or camera setups.\n",
        "\n",
        "5. **Real-time Requirements**: For production line integration, ensure inference latency meets throughput requirements (~30-50ms per image on GPU).\n",
        "\n",
        "### Mathematical Recap\n",
        "\n",
        "**YOLOv12 Loss Function:**\n",
        "\n",
        "$$\\mathcal{L}_{total} = \\lambda_{box} \\mathcal{L}_{CIoU} + \\lambda_{cls} \\mathcal{L}_{BCE} + \\lambda_{dfl} \\mathcal{L}_{DFL}$$\n",
        "\n",
        "Where:\n",
        "- $\\mathcal{L}_{CIoU}$ = Complete IoU loss for bounding box regression (considers overlap, center distance, and aspect ratio)\n",
        "- $\\mathcal{L}_{BCE}$ = Binary cross-entropy for class predictions\n",
        "- $\\mathcal{L}_{DFL}$ = Distribution focal loss for fine-grained box coordinate prediction\n",
        "\n",
        "**Confidence Threshold Selection:**\n",
        "\n",
        "$$\\text{Precision} = \\frac{TP}{TP + FP}, \\quad \\text{Recall} = \\frac{TP}{TP + FN}$$\n",
        "\n",
        "Higher threshold â†’ Higher precision, lower recall (fewer false positives, may miss defects)  \n",
        "Lower threshold â†’ Higher recall, lower precision (catch more defects, more false alarms)\n",
        "\n",
        "### Further Learning Resources\n",
        "\n",
        "- [Ultralytics YOLOv12 Documentation](https://docs.ultralytics.com/models/yolo12/)\n",
        "- [Deep PCB Dataset Paper](https://arxiv.org/abs/1902.06197)\n",
        "- [Snowflake Model Registry Documentation](https://docs.snowflake.com/en/developer-guide/snowpark-ml/model-registry/overview)\n",
        "- [SPCS GPU Inference Guide](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/overview)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Increase Training Epochs**: Re-run with 50+ epochs for production-quality model\n",
        "2. **Hyperparameter Tuning**: Experiment with learning rate, batch size, and augmentation\n",
        "3. **Deploy to SPCS**: Use `mv.deploy()` for GPU-accelerated inference endpoint\n",
        "4. **Integrate with AOI System**: Connect model predictions to manufacturing execution system\n",
        "5. **Monitor & Retrain**: Track detection accuracy over time and retrain on new defect examples\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
